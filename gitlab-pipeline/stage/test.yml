
test:backend-integration:open_source:
  tags:
    - mender-qa-worker-backend-integration-tests
  rules:
  - if: '$RUN_BACKEND_INTEGRATION_TESTS == "true"'
    when: always
  stage: test
  image: docker/compose:alpine-1.27.4
  variables:
    TEST_SUITE: "open"
  services:
    - docker:dind
  needs:
    - init:workspace
    - build:servers
    - build:mender-artifact
  before_script:
    # Default value, will later be overwritten if successful
    - echo "failure" > /JOB_RESULT.txt

    # Increase inotify limit to make sure the tests are not limited while
    # running with high parallelism on a single VM
    - sysctl -w fs.inotify.max_user_instances=1024

    # Set minimaly required by Opensearch 'max virtual memory areas'
    # https://opensearch.org/docs/2.4/install-and-configure/install-opensearch/index/#important-settings
    - sysctl -w vm.max_map_count=262144

    - docker version
    - apk --update add bash git py-pip gcc make python2-dev
      libc-dev libffi-dev openssl-dev python3 curl jq sysstat xz
    - wget https://raw.githubusercontent.com/mendersoftware/integration/master/extra/requirements.txt
    - pip3 install -r requirements.txt
    # Restore workspace from init stage
    - export WORKSPACE=$(realpath ${CI_PROJECT_DIR}/..)
    - mv workspace.tar.xz stage-artifacts /tmp
    - rm -rf ${WORKSPACE}
    - mkdir -p ${WORKSPACE}
    - cd ${WORKSPACE}
    - xz -d /tmp/workspace.tar.xz
    - tar -xf /tmp/workspace.tar
    - mv /tmp/stage-artifacts .

    # Load all docker images except client
    - for image in $(integration/extra/release_tool.py -l docker); do
    -   if ! echo $image | egrep -q 'mender-client|mender-qemu|mender-monitor|mender-gateway'; then
    -     docker load -i stage-artifacts/${image}.tar
    -   fi
    - done
    # Login for private repos
    - docker login -u menderbuildsystem -p ${DOCKER_HUB_PASSWORD}
    - docker login -u ntadm_menderci -p ${REGISTRY_MENDER_IO_PASSWORD} registry.mender.io
    # Set testing versions to PR
    - for repo in `integration/extra/release_tool.py -l docker`; do
        integration/extra/release_tool.py --set-version-of $repo --version pr;
      done
    # mender-artifact
    - mkdir -p integration/backend-tests/downloaded-tools
    - mv stage-artifacts/mender-artifact-linux integration/backend-tests/downloaded-tools/mender-artifact
    # copy for pre 2.4.x releases
    - cp integration/backend-tests/downloaded-tools/mender-artifact integration/backend-tests/mender-artifact
    - if [ -n "$MENDER_TEST_CONTAINERS_CANDIDATE_TAG" ]; then
    -   sed -i.bak -e "s,mendersoftware/mender-test-containers:backend-integration-testing,$MENDER_TEST_CONTAINERS_CANDIDATE_TAG," integration/backend-tests/docker/docker-compose.backend-tests.yml
    -   diff -u integration/backend-tests/docker/docker-compose.backend-tests.yml.bak integration/backend-tests/docker/docker-compose.backend-tests.yml || true
    -   echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" "$CI_REGISTRY" --password-stdin
    - fi
    # sysstat monitoring suite for Alpine Linux
    # collect cpu, load avg, memory and io usage every 2 secs forever
    # use 'sar' from sysstat to render the result file manually
    - ln -s /var/log/sa/ /var/log/sysstat
    - sar -P ALL 2 -o /var/log/sysstat/sysstat.log -uqrbS >/dev/null 2>&1 &
  script:
    # Traps only work if executed in a sub shell.
    - "("

    - function handle_exit() {
      ${CI_PROJECT_DIR}/scripts/maybe-wait-in-stage.sh WAIT_IN_STAGE_TEST ${CI_PROJECT_DIR}/WAIT_IN_STAGE_TEST;
      };
      trap handle_exit EXIT

    - INTEGRATION_TEST_SUITE=$(integration/extra/release_tool.py --select-test-suite || echo "all")

    - if [ "$INTEGRATION_TEST_SUITE" = "$TEST_SUITE" ] || [ "$INTEGRATION_TEST_SUITE" = "all" ]; then
        # Post job status
    -   ${CI_PROJECT_DIR}/scripts/github_pull_request_status pending "Gitlab ${CI_JOB_NAME} started" "${CI_JOB_URL}" "${CI_JOB_NAME}/${INTEGRATION_REV}"

    -   echo Running backend-tests suite $INTEGRATION_TEST_SUITE
    -   cd integration/backend-tests/

        # From 2.4.x on, the script would download the requirements by default
    -   if ./run --help | grep -e --no-download; then
    -     RUN_ARGS="--no-download";
    -   fi

        # for pre 2.2.x releases, ignore test suite selection and just run open tests
    -   if ./run --help | grep -e --suite; then
    -     ./run --suite $TEST_SUITE $RUN_ARGS;
    -   else
    -     PYTEST_ARGS="-k 'not Multitenant'" ./run;
    -   fi

        # Always keep this at the end of the script stage
    -   echo "success" > /JOB_RESULT.txt
    - else
    -   echo "skipped" > /JOB_RESULT.txt
    - fi

    - ")"

  after_script:
    - export WORKSPACE=$(realpath ${CI_PROJECT_DIR}/..)
    - if [ "$(cat /JOB_RESULT.txt)" != "skipped" ]; then
    -   if [ "$(cat /JOB_RESULT.txt)" != "success" ]; then ${CI_PROJECT_DIR}/scripts/github_pull_request_status $(cat /JOB_RESULT.txt) "Gitlab ${CI_JOB_NAME} finished" "${CI_JOB_URL}" "${CI_JOB_NAME}/${INTEGRATION_REV}"; fi

    -   find ${CI_PROJECT_DIR}/../integration/backend-tests -mindepth 1 -maxdepth 1 -name 'acceptance.*' -exec cp "{}" . \;
    -   ls ${CI_PROJECT_DIR}/../integration/backend-tests/results_*xml | xargs -n 1 -i cp {} .
    -   ls ${CI_PROJECT_DIR}/../integration/backend-tests/report_*html | xargs -n 1 -i cp {} .

    -   if [ "$NIGHTLY_BUILD" = "true" ]; then
    -     build_name=nightly-$(date +%Y-%m-%d)
    -   else
    -     build_name=pullreq-$(date +%Y-%m-%d)-${CI_PIPELINE_ID}
    -   fi
    -   if [ "$TEST_SUITE" = "open" ]; then
    -     mantra_id=$MANTRA_ID_backend_integration_open_source
    -     results_file=results_backend_integration_open.xml
    -   elif [ "$TEST_SUITE" = "enterprise" ]; then
    -     mantra_id=$MANTRA_ID_backend_integration_enterprise
    -     results_file=results_backend_integration_enterprise.xml
    -   fi
    -   ${CI_PROJECT_DIR}/scripts/mantra_post_test_results
          $mantra_id
          $build_name
          $results_file || true

    -   cp /var/log/sysstat/sysstat.log .
    -   sadf sysstat.log -g -- -qurbS > sysstat.svg

        # Post job status
    -   ${CI_PROJECT_DIR}/scripts/github_pull_request_status $(cat /JOB_RESULT.txt) "Gitlab ${CI_JOB_NAME} finished" "${CI_JOB_URL}" "${CI_JOB_NAME}/${INTEGRATION_REV}"
    - fi

  artifacts:
    expire_in: 2w
    when: always
    paths:
      - acceptance.*
      - results_backend_integration_*.xml
      - report_backend_integration_*.html
      - sysstat.log
      - sysstat.svg
    reports:
      junit: results_backend_integration_*.xml

test:backend-integration:enterprise:
  extends: test:backend-integration:open_source
  variables:
    TEST_SUITE: "enterprise"

test:backend-integration:azblob:open:
  extends: test:backend-integration:open_source
  rules:
  - if: '$RUN_BACKEND_INTEGRATION_TESTS == "true" && $CI_PIPELINE_SOURCE == "schedule"'
    when: always
  - if: '$RUN_BACKEND_INTEGRATION_TESTS == "true"'
    when: manual
    allow_failure: true
  variables:
    AZURE_STORAGE_CONTAINER_PREFIX: "backend-os"

test:backend-integration:azblob:enterprise:
  extends: test:backend-integration:enterprise
  rules:
  - if: '$RUN_BACKEND_INTEGRATION_TESTS == "true" && $CI_PIPELINE_SOURCE == "schedule"'
    when: always
  - if: '$RUN_BACKEND_INTEGRATION_TESTS == "true"'
    when: manual
    allow_failure: true
  variables:
    AZURE_STORAGE_CONTAINER_PREFIX: "backend-ent"

.integration_setup_template:
  tags:
    - mender-qa-worker-integration-tests
  stage: test
  # Integration tests depends on running ssh to containers, we're forced to
  # run dockerd on the same host. It prevents us from using GitLab service,
  # because technically it runs dockerd in a separate container, interacts
  # with it via tcp and all containers starts in that separate container
  # when with used by GitLab network driver network connection between those
  # containers is denied (it might work locally on Linux host because of the
  # default bridge driver).
  image: docker:24.0.7-dind-alpine3.18
  variables:
    DOCKER_CLIENT_TIMEOUT: 300
    COMPOSE_HTTP_TIMEOUT: 300
    TEST_SUITE: "open"
  before_script:
    # Default value, will later be overwritten if successful
    - echo "failure" > /JOB_RESULT.txt

    # Make sure the /dev/kvm device is readable and writable by everyone
    - chmod o+rw /dev/kvm

    # Increase inotify limit to make sure the tests are not limited while
    # running with high parallelism on a single VM
    - sysctl -w fs.inotify.max_user_instances=1024

    # Set minimaly required by Opensearch 'max virtual memory areas'
    # https://opensearch.org/docs/2.4/install-and-configure/install-opensearch/index/#important-settings
    - sysctl -w vm.max_map_count=262144

    # Restore workspace from init stage
    - export WORKSPACE=$(realpath ${CI_PROJECT_DIR}/..)
    - apk add xz
    - mv workspace.tar.xz stage-artifacts /tmp
    - rm -rf ${WORKSPACE}
    - mkdir -p ${WORKSPACE}
    - cd ${WORKSPACE}
    - xz -d /tmp/workspace.tar.xz
    - tar -xf /tmp/workspace.tar
    - mv /tmp/stage-artifacts .

    # Dependencies for post job status and io stats
    - apk --update add curl jq sysstat hdparm

    # Make sure docker clients (cli and python) interacts with dockerd via socket
    - unset DOCKER_HOST
    - unset DOCKER_TLS_VERIFY
    - unset DOCKER_CERT_PATH

    # Start dockerd in the background
    - export TINI_SUBREAPER=true # https://github.com/krallin/tini#subreaping
    - |-
      # The entrypoint script additionally uses docker-init as workarround of known
      # containerd issue - https://github.com/docker-library/docker/issues/318
      run_dockerd() {
        /bin/sh -c '/usr/local/bin/dockerd-entrypoint.sh &>${CI_PROJECT_DIR}/dockerd.log &'
      }
      # Run dockerd and wait it to start
      MAX_WAIT=60
      while [ ${MAX_WAIT} -gt 0 ]; do
        echo "[$(date +%F_%T)] MAX_WAIT=${MAX_WAIT}"; ps # Debug information
        if docker version &>/dev/null; then
          docker version # Verify that the dockerd is up and running
          break
        elif ! ps -o comm | grep -q -E "docker-init|dockerd|containerd|dockerd-entry|dind|openssl"; then
          run_dockerd # Run dockerd if no related processes are running
        fi
        MAX_WAIT=$((${MAX_WAIT} - 1))
        sleep 1
      done

    # Output storage io stats
    - df -h . | tail -1 | awk '{system("hdparm -tT "$1);}'

    # Get and install the integration test requirements
    - if [ -d ${WORKSPACE}/integration/tests/requirements-python ]; then
    -   apk add $(cat ${WORKSPACE}/integration/tests/requirements-system/apk-requirements.txt)
    -   pip install --break-system-packages -r ${WORKSPACE}/integration/tests/requirements-python/python-requirements.txt
    - else
    -   apk add $(cat ${WORKSPACE}/integration/tests/requirements/apk-requirements.txt)
    -   pip install --break-system-packages -r ${WORKSPACE}/integration/tests/requirements/python-requirements.txt
    - fi

    # Load all docker images, and the client images depending on $BUILD_CLIENT
    - for image in $(integration/extra/release_tool.py -l docker); do
    -   if echo $image | egrep -q 'mender-client|mender-qemu|mender-monitor|mender-gateway'; then
    -     if [ "${BUILD_CLIENT}" = "true" ]; then
    -       docker load -i stage-artifacts/${image}.tar
    -     else
    -       continue
    -     fi
    -   fi
    -   docker load -i stage-artifacts/${image}.tar
    - done

    # Login for private repos
    - docker login -u menderbuildsystem -p ${DOCKER_HUB_PASSWORD}
    - docker login -u ntadm_menderci -p ${REGISTRY_MENDER_IO_PASSWORD} registry.mender.io
    # Set testing versions to PR
    - for image in $(integration/extra/release_tool.py -l docker); do
    -   if echo $image | egrep -q 'mender-client|mender-qemu|mender-monitor|mender-gateway'; then
    -     if [ "${BUILD_CLIENT}" = "true" ]; then
    -       integration/extra/release_tool.py --set-version-of $image --version pr
    -     else
    -       continue
    -     fi
    -   fi
    -   integration/extra/release_tool.py --set-version-of $image --version pr
    - done
    # Other dependencies
    - install stage-artifacts/mender-artifact-linux /usr/local/bin/mender-artifact
    - install stage-artifacts/mender-cli.linux.amd64 /usr/local/bin/mender-cli

    - ( cd ${WORKSPACE}/go/src/github.com/mendersoftware/mender && git submodule update --init )
    # New-style C++ client build. If this fails for any reason, then the next line should also fail,
    # so using `|| true` should be ok. With the Golang client, this will always fail, but it has its
    # own Makefile which will be picked up on the next line.
    - cmake -S ${WORKSPACE}/go/src/github.com/mendersoftware/mender -B ${WORKSPACE}/go/src/github.com/mendersoftware/mender -D MENDER_NO_BUILD=1 || true
    # Install the artifact module generators
    - make -C ${WORKSPACE}/go/src/github.com/mendersoftware/mender install-modules-gen

    # sysstat monitoring suite for Alpine Linux
    # collect cpu, load avg, memory and io usage every 2 secs forever
    # use 'sar' from sysstat to render the result file manually
    - ln -s /var/log/sa/ /var/log/sysstat
    - sar -P ALL 2 -o /var/log/sysstat/sysstat.log -uqrbS >/dev/null 2>&1 &
  script:
    # Traps only work if executed in a sub shell.
    - "("

    - function handle_exit() {
      ${CI_PROJECT_DIR}/scripts/maybe-wait-in-stage.sh WAIT_IN_STAGE_TEST ${CI_PROJECT_DIR}/WAIT_IN_STAGE_TEST;
      };
      trap handle_exit EXIT

    - INTEGRATION_TEST_SUITE=$(integration/extra/release_tool.py --select-test-suite || echo "all")
    - if [ "$INTEGRATION_TEST_SUITE" = "$TEST_SUITE" ] || [ "$INTEGRATION_TEST_SUITE" = "all" ]; then
        # Post job status
    -   ${CI_PROJECT_DIR}/scripts/github_pull_request_status pending "Gitlab ${CI_JOB_NAME} started" "${CI_JOB_URL}" "${CI_JOB_NAME}/${INTEGRATION_REV}"

    -   echo Running integration tests suite $INTEGRATION_TEST_SUITE
        # only do automatic test suite selection if the user wasn't specific
        # run.sh will pick up the SPECIFIC_INTEGRATION_TEST var
    -   if [ -z "$SPECIFIC_INTEGRATION_TEST" ]; then
          case $TEST_SUITE in
            "enterprise")
              export SPECIFIC_INTEGRATION_TEST="Enterprise";;
            "open")
              export SPECIFIC_INTEGRATION_TEST="not Enterprise";;
          esac
        fi
    -   cd integration/tests
    -   ./run.sh --no-download -- --machine-name qemux86-64

        # Always keep this at the end of the script stage
    -   echo "success" > /JOB_RESULT.txt
    - else
    -   echo "skipped" > /JOB_RESULT.txt
    - fi

    - ")"

  after_script:
    # Make sure docker clients (cli and python) interacts with dockerd via socket
    - unset DOCKER_HOST
    - unset DOCKER_TLS_VERIFY
    - unset DOCKER_CERT_PATH

    - export WORKSPACE=$(realpath ${CI_PROJECT_DIR}/..)
    - if [ "$(cat /JOB_RESULT.txt)" != "skipped" ]; then
    -   if [ "$(cat /JOB_RESULT.txt)" != "success" ]; then ${CI_PROJECT_DIR}/scripts/github_pull_request_status $(cat /JOB_RESULT.txt) "Gitlab ${CI_JOB_NAME} finished" "${CI_JOB_URL}" "${CI_JOB_NAME}/${INTEGRATION_REV}"; fi

    -   cp -r ${CI_PROJECT_DIR}/../integration/tests/mender_test_logs .
    -   cp ${CI_PROJECT_DIR}/../integration/tests/results.xml results_full_integration.xml
    -   cp ${CI_PROJECT_DIR}/../integration/tests/report.html report_full_integration.html

    -   if [ "$NIGHTLY_BUILD" = "true" ]; then
    -     build_name=nightly-$(date +%Y-%m-%d)
    -   else
    -     build_name=pullreq-$(date +%Y-%m-%d)-${CI_PIPELINE_ID}
    -   fi
    -   if [ "$TEST_SUITE" = "open" ]; then
    -     mantra_id=$MANTRA_ID_full_integration_open_source
    -   elif [ "$TEST_SUITE" = "enterprise" ]; then
    -     mantra_id=$MANTRA_ID_full_integration_enterprise
    -   fi
    -   ${CI_PROJECT_DIR}/scripts/mantra_post_test_results
          $mantra_id
          $build_name
          results_full_integration.xml || true

    -   cp /var/log/sysstat/sysstat.log .
    -   sadf sysstat.log -g -- -qurbS > sysstat.svg

        # Post job status
    -   ${CI_PROJECT_DIR}/scripts/github_pull_request_status $(cat /JOB_RESULT.txt) "Gitlab ${CI_JOB_NAME} finished" "${CI_JOB_URL}" "${CI_JOB_NAME}/${INTEGRATION_REV}"
    - fi

  artifacts:
    expire_in: 2w
    when: always
    paths:
      - mender_test_logs
      - results_full_integration.xml
      - report_full_integration.html
      - sysstat.log
      - sysstat.svg
      - dockerd.log
    reports:
      junit: results_full_integration.xml


test:integration:source_client:open_source:
  extends: .integration_setup_template
  only:
    variables:
      - ( $RUN_INTEGRATION_TESTS == "true" && $BUILD_CLIENT == "true" )
  needs:
    - init:workspace
    - build:servers
    - build:client:qemu
    - build:client:docker
    - build:mender-artifact
    - build:mender-cli


test:integration:source_client:enterprise:
  extends: test:integration:source_client:open_source
  variables:
    TEST_SUITE: "enterprise"


test:integration:prebuilt_client:open_source:
  extends: .integration_setup_template
  only:
    variables:
      - $RUN_INTEGRATION_TESTS == "true"
  except:
    variables:
      - $BUILD_CLIENT == "true"
  needs:
    - init:workspace
    - build:servers
    - build:mender-artifact
    - build:mender-cli


test:integration:prebuilt_client:enterprise:
  extends: test:integration:prebuilt_client:open_source
  variables:
    TEST_SUITE: "enterprise"
